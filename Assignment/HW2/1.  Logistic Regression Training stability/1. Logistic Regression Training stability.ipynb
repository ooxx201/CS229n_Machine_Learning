{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lr_debug import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (a)\n",
    "The logistic_regression is able to converge on dataset a while cannot converge on dataset b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "itr = 10000 * 10\n",
    "\n",
    "def logistic_regression_1(X, Y):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    learning_rate = 10\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        prev_theta = theta\n",
    "        grad = calc_grad(X, Y, theta)\n",
    "        theta = theta  - learning_rate * (grad)\n",
    "        norm = np.linalg.norm(prev_theta - theta)\n",
    "        \n",
    "        predict = np.array([1. if i >= 0 else -1. for i in X.dot(theta)])\n",
    "        precision = np.sum(predict == Y) / Y.shape[0]\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print('Finished {0} iterations; Diff theta: {1}; theta: {2}; Grad: {3}; Precision: {4}'.format(\n",
    "                i, norm, theta, grad, precision))\n",
    "            #import pdb; pdb.set_trace()\n",
    "        if norm < 1e-15:\n",
    "            print('Converged in %d iterations' % i)\n",
    "            break\n",
    "        if i >= itr:\n",
    "            print('Not Converged in %d iterations' % i)\n",
    "            break\n",
    "    return\n",
    "\n",
    "def main_1():\n",
    "    print('==== Training model on data set A ====')\n",
    "    Xa, Ya = load_data('data_a.txt')\n",
    "    logistic_regression_1(Xa, Ya)\n",
    "\n",
    "    print('\\n==== Training model on data set B ====')\n",
    "    Xb, Yb = load_data('data_b.txt')\n",
    "    logistic_regression_1(Xb, Yb)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Training model on data set A ====\n",
      "Finished 10000 iterations; Diff theta: 7.226491864936692e-07; theta: [-20.81394174  21.45250215  19.85155266]; Grad: [ 4.15154546e-08 -4.27822247e-08 -4.08456455e-08]; Precision: 0.92\n",
      "Finished 20000 iterations; Diff theta: 5.3329785269148335e-11; theta: [-20.81437785  21.45295156  19.85198173]; Grad: [ 3.06367123e-12 -3.15717157e-12 -3.01431501e-12]; Precision: 0.92\n",
      "Finished 30000 iterations; Diff theta: 6.153480596427404e-15; theta: [-20.81437788  21.45295159  19.85198176]; Grad: [ 1.93642961e-16 -2.87477322e-16 -1.92518176e-16]; Precision: 0.92\n",
      "Converged in 30372 iterations\n",
      "\n",
      "==== Training model on data set B ====\n",
      "Finished 10000 iterations; Diff theta: 0.003361039469518825; theta: [-52.74109217  52.92982273  52.69691453]; Grad: [ 0.00019399 -0.00019355 -0.00019461]; Precision: 1.0\n",
      "Finished 20000 iterations; Diff theta: 0.002173205351041188; theta: [-68.10040977  68.26496086  68.09888223]; Grad: [ 0.00012541 -0.00012529 -0.0001257 ]; Precision: 1.0\n",
      "Finished 30000 iterations; Diff theta: 0.0016644896054955345; theta: [-79.01759142  79.17745526  79.03755803]; Grad: [ 9.60445104e-05 -9.60553587e-05 -9.61981110e-05]; Precision: 1.0\n",
      "Finished 40000 iterations; Diff theta: 0.00137040952812708; theta: [-87.70771189  87.87276307  87.73897393]; Grad: [ 7.90651647e-05 -7.91523106e-05 -7.91443884e-05]; Precision: 1.0\n",
      "Finished 50000 iterations; Diff theta: 0.0011758957242939641; theta: [-95.01838735  95.1948202   95.0551918 ]; Grad: [ 6.78328688e-05 -6.79700947e-05 -6.78680755e-05]; Precision: 1.0\n",
      "Finished 60000 iterations; Diff theta: 0.0010366393703263115; theta: [-101.37921493  101.57119731  101.41805781]; Grad: [ 5.97905761e-05 -5.99622968e-05 -5.97981756e-05]; Precision: 1.0\n",
      "Finished 70000 iterations; Diff theta: 0.0009315199306059815; theta: [-107.04156569  107.25200975  107.08020705]; Grad: [ 5.37193268e-05 -5.39154728e-05 -5.37089335e-05]; Precision: 1.0\n",
      "Finished 80000 iterations; Diff theta: 0.0008490730225995943; theta: [-112.16638881  112.39737225  112.20335022]; Grad: [ 4.89573832e-05 -4.91710659e-05 -4.89349658e-05]; Precision: 1.0\n",
      "Finished 90000 iterations; Diff theta: 0.0007824983080599451; theta: [-116.86340448  117.11642203  116.89769046]; Grad: [ 4.51121714e-05 -4.53384732e-05 -4.50816023e-05]; Precision: 1.0\n",
      "Finished 100000 iterations; Diff theta: 0.0007274911705015017; theta: [-121.21097234  121.48709579  121.24190606]; Grad: [ 4.19351585e-05 -4.21704526e-05 -4.18990387e-05]; Precision: 1.0\n",
      "Not Converged in 100000 iterations\n"
     ]
    }
   ],
   "source": [
    "main_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data B is perfect linearly separable, the model will keep increasing scale of theta to increase likehood  $|\\theta^Tx|$. While the maximum likehood is 1, which is unreachable for logistic regression model, thus theta will never stop increasing till inifite.  \n",
    "* Data A is not linearly separable, the maximum likehood is smaller than 1. Logistic regression model will converge after reaching maximum likehood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Using a different constant learning rate.  \n",
    "* **No**, the model will keep increasing likehood with any learning rate when the data is perfect linearly separable.   \n",
    "\n",
    "ii. Decreasing the learning rate over time. (e.g. scaling the initial learning rate by $1/t^2$, where $t$ is the number of gradient descent iterations thus far)\n",
    "* **No**, the learning process will not converge if the learning rate decrease over time.  \n",
    "\n",
    "iii. Adding a regularization term $\\left|\\left|\\theta\\right|\\right|^2_2$ to the loss function.\n",
    "* **Yes**, the will prevent theta form become larger.\n",
    "\n",
    "iv. Linear scaling of the input features.\n",
    "* **No**, scaling input features is irrelevant to $\\theta$ getting infinitely large.\n",
    "\n",
    "v. Adding zero-mean Gaussian noise to the training data or labels.\n",
    "* **Yes**, adding Gaussian noise may make the training data imperfectly separable with a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIND",
   "language": "python",
   "name": "aind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
