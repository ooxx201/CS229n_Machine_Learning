{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.0600\n"
     ]
    }
   ],
   "source": [
    "import nb\n",
    "nb.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['httpaddr', 'spam', 'unsubscrib', 'ebai', 'valet'], dtype='<U16')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nb import nb_train, readMatrix, nb_test\n",
    "import numpy as np\n",
    "\n",
    "# Read file\n",
    "trainMatrix, tokenlist, trainCategory = nb.readMatrix('spam_data/MATRIX.TRAIN') \n",
    "\n",
    "# Train\n",
    "state = nb_train(trainMatrix, trainCategory)\n",
    "\n",
    "# Get  indicative tokens\n",
    "log_phi_0, log_phi_1, log_p_0, log_p_1 = state\n",
    "log_ratio = log_phi_1 - log_phi_0\n",
    "indices = (log_ratio).argsort().astype(int)[-5:][::-1]\n",
    "indicate_tokens = np.array(tokenlist)[indices]\n",
    "indicate_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size 50: Test set error = 0.21%.\n",
      "Training set size 100: Test set error = 0.21%.\n",
      "Training set size 200: Test set error = 0.20%.\n",
      "Training set size 400: Test set error = 0.09%.\n",
      "Training set size 800: Test set error = 0.06%.\n",
      "Training set size 1400: Test set error = 0.05%.\n",
      "Full training set: Test set error = 0.06%.\n"
     ]
    }
   ],
   "source": [
    "def run_nb(size):\n",
    "    train_path = 'spam_data/MATRIX.TRAIN'\n",
    "    test_path = 'spam_data/MATRIX.TEST'\n",
    "    if size > 0:\n",
    "        train_path = '%s.%d'% (train_path, size)\n",
    "        \n",
    "    trainMatrix, tokenlist, trainCategory = nb.readMatrix(train_path)\n",
    "    testMatrix, tokenlist, testCategory = nb.readMatrix(test_path)\n",
    "\n",
    "    state = nb_train(trainMatrix, trainCategory)\n",
    "    output = nb_test(testMatrix, state)\n",
    "\n",
    "    return (output != testCategory).sum() * 1. / len(output)\n",
    "\n",
    "sizeList = [50, 100, 200, 400, 800, 1400, -1]\n",
    "\n",
    "for size in sizeList:\n",
    "    error = run_nb(size)\n",
    "    if size > 0:\n",
    "        print('Training set size %d: Test set error = %1.2f%%.'% (size, error))\n",
    "    else:\n",
    "        print('Full training set: Test set error = %1.2f%%.'% (error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size 50: Test set error = 0.01%.\n",
      "Training set size 100: Test set error = 0.01%.\n",
      "Training set size 200: Test set error = 0.00%.\n",
      "Training set size 400: Test set error = 0.00%.\n",
      "Training set size 800: Test set error = 0.00%.\n",
      "Training set size 1400: Test set error = 0.00%.\n",
      "Full training set: Test set error = 0.00%.\n"
     ]
    }
   ],
   "source": [
    "import svm\n",
    "from svm import svm_train, svm_test \n",
    "\n",
    "def run_svm(size):\n",
    "    train_path = 'spam_data/MATRIX.TRAIN'\n",
    "    test_path = 'spam_data/MATRIX.TEST'\n",
    "    if size > 0:\n",
    "        train_path = '%s.%d'% (train_path, size)\n",
    "        \n",
    "    trainMatrix, tokenlist, trainCategory = svm.readMatrix(train_path)\n",
    "    testMatrix, tokenlist, testCategory = svm.readMatrix(test_path)\n",
    "\n",
    "    state = svm_train(trainMatrix, trainCategory)\n",
    "    output = svm_test(testMatrix, state)\n",
    "\n",
    "    return (output != testCategory).sum() * 1. / len(output)\n",
    "\n",
    "for size in sizeList:\n",
    "    error = run_svm(size)\n",
    "    if size > 0:\n",
    "        print('Training set size %d: Test set error = %1.2f%%.'% (size, error))\n",
    "    else:\n",
    "        print('Full training set: Test set error = %1.2f%%.'% (error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)\n",
    "* For each training data set size, SVM have better performance. When Naive Bayes fit the model a litele faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIND",
   "language": "python",
   "name": "aind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
